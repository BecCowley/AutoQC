{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Learning Investigation\n",
    "\n",
    "Here we explore some off-the-shelf learning strategies for identifying datasets to be flagged based on the results of the AutoQC suite of tests.\n",
    "\n",
    " - **AutoQC Version:** `learning-0.0.1`\n",
    " - **Dataset:** full quota dataset (unreleased)\n",
    " - **OS:** osx 10.10.3\n",
    " - **python:** Python 2.7.10 :: Anaconda 2.3.0 (x86_64); see `pip_freeze.dat` for package versions.\n",
    " \n",
    "## 0.1: Data Reduction\n",
    "\n",
    "The full dataset was processed by AutoQC, and the results logged as JSON serializations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def reloadData(): \n",
    "    ## read raw data\n",
    "    with open('../../../AutoQC_raw/true.dat') as true_data:    \n",
    "        truth = json.load(true_data)\n",
    "\n",
    "    with open('../../../AutoQC_raw/results.dat') as results_data:    \n",
    "        rawResults = json.load(results_data)\n",
    "        \n",
    "    return truth, rawResults\n",
    "\n",
    "truth, rawResults = reloadData()\n",
    "datasetSize = len(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2: Helpers\n",
    "\n",
    "A few helpers and parameters are defined here for subsequent use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def shuffleLists(a, b):\n",
    "  '''\n",
    "  given two lists a, b, shuffle them maintaining pairwise correspondence.\n",
    "  thanks http://stackoverflow.com/questions/13343347/randomizing-two-lists-and-maintaining-order-in-python\n",
    "  '''\n",
    "\n",
    "  combined = zip(a, b)\n",
    "  random.seed(2154)\n",
    "  random.shuffle(combined)\n",
    "\n",
    "  a[:], b[:] = zip(*combined)\n",
    "\n",
    "def transpose(lists):\n",
    "    '''\n",
    "    return the transpose of lists, a list of lists.\n",
    "    all the inner lists had better be the same length!\n",
    "    '''\n",
    "\n",
    "    T = []\n",
    "    for i in range(len(lists[0])):\n",
    "        T.append([None]*len(lists))\n",
    "\n",
    "    for i in range(len(lists)):\n",
    "        for j in range(len(lists[0])):\n",
    "            T[j][i] = lists[i][j]\n",
    "\n",
    "    return T\n",
    "\n",
    "def runClassifier(classifier, trainingSize):\n",
    "    '''\n",
    "    given a scikit-learn classifier, train it on the first trainingSize points of data and truth,\n",
    "    and return the prediction classes on the remainder of data\n",
    "    '''\n",
    "    #load and arrange data\n",
    "    truth, rawResults = reloadData()\n",
    "    data = transpose(rawResults) #arrange data into rows by profile for consumption by scikit-learn\n",
    "    shuffleLists(data, truth)    #randomize order of profiles\n",
    "    \n",
    "    #train svm\n",
    "    classifier.fit(data[0:trainingSize], truth[0:trainingSize])\n",
    "\n",
    "    #predict values for remainder of profiles\n",
    "    TT = 0.\n",
    "    TF = 0.\n",
    "    FT = 0.\n",
    "    FF = 0.\n",
    "\n",
    "    for i in range(trainingSize, len(truth)):\n",
    "        assessment = classifier.predict(data[i])\n",
    "        if assessment and truth[i]:\n",
    "            TT += 1\n",
    "        elif assessment and not truth[i]:\n",
    "            TF += 1\n",
    "        elif not assessment and truth[i]:\n",
    "            FT += 1\n",
    "        elif not assessment and not truth[i]:\n",
    "            FF += 1  \n",
    "            \n",
    "    return TT, TF, FT, FF\n",
    "\n",
    "trainingSize = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3: Perfomance to Beat\n",
    "\n",
    "To start, we consider the performance of each of the 13 tests implemented in `learning-0.0.1` independently; that is, how would they perform in the absense of all other information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\t Correct flags: 0.0158228863769\n",
      "\t False positive: 0.000609806980043\n",
      "\t False negative: 0.100836398416\n",
      "\t Correct pass: 0.882730908227\n",
      "1\n",
      "\t Correct flags: 0.0269021163512\n",
      "\t False positive: 0.000898662917958\n",
      "\t False negative: 0.0897571684415\n",
      "\t Correct pass: 0.882442052289\n",
      "2\n",
      "\t Correct flags: 3.20951042128e-05\n",
      "\t False positive: 0.000629064042571\n",
      "\t False negative: 0.116627189688\n",
      "\t Correct pass: 0.882711651165\n",
      "3\n",
      "\t Correct flags: 0.0\n",
      "\t False positive: 0.0\n",
      "\t False negative: 0.116659284793\n",
      "\t Correct pass: 0.883340715207\n",
      "4\n",
      "\t Correct flags: 0.00683625719733\n",
      "\t False positive: 0.0413962654137\n",
      "\t False negative: 0.109823027595\n",
      "\t Correct pass: 0.841944449794\n",
      "5\n",
      "\t Correct flags: 0.00100136725144\n",
      "\t False positive: 0.00478858954855\n",
      "\t False negative: 0.115657917541\n",
      "\t Correct pass: 0.878552125659\n",
      "6\n",
      "\t Correct flags: 0.00654740125941\n",
      "\t False positive: 7.06092292682e-05\n",
      "\t False negative: 0.110111883533\n",
      "\t Correct pass: 0.883270105978\n",
      "7\n",
      "\t Correct flags: 0.0571806376655\n",
      "\t False positive: 0.0430202776868\n",
      "\t False negative: 0.0594786471272\n",
      "\t Correct pass: 0.84032043752\n",
      "8\n",
      "\t Correct flags: 0.000609806980043\n",
      "\t False positive: 0.000378722229711\n",
      "\t False negative: 0.116049477813\n",
      "\t Correct pass: 0.882961992978\n",
      "9\n",
      "\t Correct flags: 0.0144813110208\n",
      "\t False positive: 0.000609806980043\n",
      "\t False negative: 0.102177973772\n",
      "\t Correct pass: 0.882730908227\n",
      "10\n",
      "\t Correct flags: 0.00476291346518\n",
      "\t False positive: 0.000192570625277\n",
      "\t False negative: 0.111896371328\n",
      "\t Correct pass: 0.883148144582\n",
      "11\n",
      "\t Correct flags: 0.032210646588\n",
      "\t False positive: 0.0204317433419\n",
      "\t False negative: 0.0844486382047\n",
      "\t Correct pass: 0.862908971865\n",
      "12\n",
      "\t Correct flags: 0.0320373330252\n",
      "\t False positive: 0.00124529004346\n",
      "\t False negative: 0.0846219517675\n",
      "\t Correct pass: 0.882095425164\n"
     ]
    }
   ],
   "source": [
    "def printSummary(title, TT, TF, FT, FF):\n",
    "    print title\n",
    "    print '\\t Correct flags:', TT\n",
    "    print '\\t False positive:', TF\n",
    "    print '\\t False negative:', FT\n",
    "    print '\\t Correct pass:', FF\n",
    "\n",
    "for i in range(len(rawResults)):\n",
    "    TT = 0.\n",
    "    TF = 0.\n",
    "    FT = 0.\n",
    "    FF = 0.\n",
    "    for j in range(len(rawResults[i])):\n",
    "        if rawResults[i][j] and truth[j]:\n",
    "            TT += 1\n",
    "        elif rawResults[i][j] and not truth[j]:\n",
    "            TF += 1\n",
    "        elif not rawResults[i][j] and truth[j]:\n",
    "            FT += 1\n",
    "        elif not rawResults[i][j] and not truth[j]:\n",
    "            FF +=1\n",
    "    printSummary(i, TT/len(truth), TF/len(truth), FT/len(truth), FF/len(truth))\n",
    "    #print i, ': ', TT/len(truth), TF/len(truth), FT/len(truth), FF/len(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row 7, corrsponding to the `EN_background_check` test, gives the best standalone performance, with around 5.7% of the entire dataset providing correct flags and and another 5.9% providing false negatives, and about 4.3% identified as false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Individual Classifiers\n",
    "\n",
    "In this section, we explore several of the individual classifiers presented by scikit-learn. We attempt to remain as parameter-agnostic as possible at this stage, using defaults wherever possible.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Several of the tecniques investigated below produce classifiers that flag about half of all datasets that should be flagged, with small false positive rates. The classifiers that stand out the most are:\n",
    "\n",
    " - SVM with linear kernel\n",
    " - linear discriminant analysis\n",
    " - quadratic disciminant analysis\n",
    " - kernel ridge\n",
    " - SGD with hinge losses\n",
    " - SGD with logistic loss & elasticnet penalty\n",
    " - k=1000 NN classifier with distance-weighted polling\n",
    " - nearest centroid\n",
    " - decision trees\n",
    " \n",
    "These classifiers will form the basis of further inquiry in the next section, on ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Support Vector Machines\n",
    "\n",
    "First we examine the performance of [scikit-learn's SVM](http://scikit-learn.org/stable/modules/svm.html), with default kernels. Notably, randomization of data order was necessary before SVM training, to ensure no systematics from sorting. Substantially worse performance of this classifier was observed when trained on non-randomized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with linear kernel\n",
      "\t Correct flags: 0.054825681259\n",
      "\t False positive: 0.0187814599402\n",
      "\t False negative: 0.0617360913076\n",
      "\t Correct pass: 0.864656767493\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#linear kernel\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(svm.SVC(kernel='linear'), trainingSize)\n",
    "printSummary('SVM with linear kernel', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with polynomial kernel\n",
      "\t Correct flags: 0.0262555790619\n",
      "\t False positive: 0.000258642986464\n",
      "\t False negative: 0.0903061935047\n",
      "\t Correct pass: 0.883179584447\n"
     ]
    }
   ],
   "source": [
    "#polynomial kernel\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(svm.SVC(kernel='poly'), trainingSize)\n",
    "printSummary('SVM with polynomial kernel', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with rbf kernel\n",
      "\t Correct flags: 0.0329471373527\n",
      "\t False positive: 0.00179723716235\n",
      "\t False negative: 0.0836146352139\n",
      "\t Correct pass: 0.881640990271\n"
     ]
    }
   ],
   "source": [
    "#rbf kernel\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(svm.SVC(kernel='rbf'), trainingSize)\n",
    "printSummary('SVM with rbf kernel', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with sigmoid kernel\n",
      "\t Correct flags: 0.0\n",
      "\t False positive: 0.0\n",
      "\t False negative: 0.116561772567\n",
      "\t Correct pass: 0.883438227433\n"
     ]
    }
   ],
   "source": [
    "#sigmoid kernel\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(svm.SVC(kernel='sigmoid'), trainingSize)\n",
    "printSummary('SVM with sigmoid kernel', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the out-of-the box linear SVM performs comparably to `EN_background`, but with a lower false positive rate. Naively, the SVM is learning that `EN_background` is the best predictor, and uses other tests to veto some false positives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Linear & Quadratic Discriminant Analysis\n",
    "\n",
    "Next we explore the [discriminant analysis techniques](http://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html#example-classification-plot-lda-qda-py) presented by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear discriminant\n",
      "\t Correct flags: 0.0549848461737\n",
      "\t False positive: 0.0198226637575\n",
      "\t False negative: 0.0615769263929\n",
      "\t Correct pass: 0.863615563676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.lda import LDA\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(LDA(solver=\"svd\"), trainingSize)\n",
    "printSummary('Linear discriminant', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic discriminant\n",
      "\t Correct flags: 0.0637588120992\n",
      "\t False positive: 0.0839130694291\n",
      "\t False negative: 0.0528029604674\n",
      "\t Correct pass: 0.799525158004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.qda import QDA\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(QDA(), trainingSize)\n",
    "printSummary('Quadratic discriminant', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see the linear discriminant behaves very comparably to the linear SVM, while the quadratic disciminant gives the best flagging performance so far, albeit at the cost of a substantially higher false positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Kernel Ridge\n",
    "\n",
    "Given the efficacy of the SVM, other kernel-trick based algorithms are worth exploring; here we try the [kernel ridge algorithm](http://scikit-learn.org/stable/modules/kernel_ridge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Ridge with linear kernel\n",
      "\t Correct flags: 0.0637919714564\n",
      "\t False positive: 0.0845563609595\n",
      "\t False negative: 0.0527698011102\n",
      "\t Correct pass: 0.798881866474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(KernelRidge(kernel='linear'), trainingSize)\n",
    "printSummary('Kernel Ridge with linear kernel', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4: Stochastic Gradient Decent\n",
    "\n",
    "Next we consider the [SGD algorithm](http://scikit-learn.org/stable/modules/sgd.html) for learning classification, exploring a few possible configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD with hinge loss & L2 penalty\n",
      "\t Correct flags: 0.0549649505594\n",
      "\t False positive: 0.018927361112\n",
      "\t False negative: 0.0615968220072\n",
      "\t Correct pass: 0.864510866321\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(SGDClassifier(loss=\"hinge\", penalty=\"l2\"), trainingSize)\n",
    "printSummary('SGD with hinge loss & L2 penalty', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD with hinge loss & elasticnet penalty\n",
      "\t Correct flags: 0.0550047417881\n",
      "\t False positive: 0.0191462128698\n",
      "\t False negative: 0.0615570307785\n",
      "\t Correct pass: 0.864292014564\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(SGDClassifier(loss=\"hinge\", penalty=\"elasticnet\"), trainingSize)\n",
    "printSummary('SGD with hinge loss & elasticnet penalty', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD with modified huber loss & L2 penalty\n",
      "\t Correct flags: 0.033291994668\n",
      "\t False positive: 0.00207577576316\n",
      "\t False negative: 0.0832697778986\n",
      "\t Correct pass: 0.88136245167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(SGDClassifier(loss=\"modified_huber\", penalty=\"l2\"), trainingSize)\n",
    "printSummary('SGD with modified huber loss & L2 penalty', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD with modified huber loss & elasticnet penalty\n",
      "\t Correct flags: 0.0331527253676\n",
      "\t False positive: 0.00185029213394\n",
      "\t False negative: 0.083409047199\n",
      "\t Correct pass: 0.881587935299\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(SGDClassifier(loss=\"modified_huber\", penalty=\"elasticnet\"), trainingSize)\n",
    "printSummary('SGD with modified huber loss & elasticnet penalty', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD with logistic loss & L2 penalty\n",
      "\t Correct flags: 0.0327680768236\n",
      "\t False positive: 0.00152533043299\n",
      "\t False negative: 0.083793695743\n",
      "\t Correct pass: 0.881912897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(SGDClassifier(loss=\"log\", penalty=\"l2\"), trainingSize)\n",
    "printSummary('SGD with logistic loss & L2 penalty', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD with logistic loss & elasticnet penalty\n",
      "\t Correct flags: 0.0548522087448\n",
      "\t False positive: 0.0187880918116\n",
      "\t False negative: 0.0617095638218\n",
      "\t Correct pass: 0.864650135622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(SGDClassifier(loss=\"log\", penalty=\"elasticnet\"), trainingSize)\n",
    "printSummary('SGD with logistic loss & elasticnet penalty', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best of these are comparable to the linear kernel SVM or the linear discriminant analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5: Nearest Neighbor Classification\n",
    "\n",
    "Next we explore [kNN](http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification) classification; we restrict ourselves to k nearest neighbour techniques, as the dimensionality of the inputs space is large (and growing). We explore a logarithmic range of k values, to get a gross sense of the effect of this choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN, k=10\n",
      "\t Correct flags: 0.0337562256693\n",
      "\t False positive: 0.00129321493232\n",
      "\t False negative: 0.0828055468973\n",
      "\t Correct pass: 0.882145012501\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(neighbors.KNeighborsClassifier(10, weights='uniform'), trainingSize)\n",
    "printSummary('kNN, k=10', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN, k=100\n",
      "\t Correct flags: 0.0314947575056\n",
      "\t False positive: 0.000848879545319\n",
      "\t False negative: 0.085067015061\n",
      "\t Correct pass: 0.882589347888\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(neighbors.KNeighborsClassifier(100, weights='uniform'), trainingSize)\n",
    "printSummary('kNN, k=100', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN, k=1000\n",
      "\t Correct flags: 0.0\n",
      "\t False positive: 0.0\n",
      "\t False negative: 0.116561772567\n",
      "\t Correct pass: 0.883438227433\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(neighbors.KNeighborsClassifier(1000, weights='uniform'), trainingSize)\n",
    "printSummary('kNN, k=1000', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps unsurprisingly, increasing k pushes the algorithm to never raise a flag; for large k, NN essentially takes the majortiy result of the dataset, which is mostly no-flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN, k=10, distance weighted\n",
      "\t Correct flags: 0.034180665442\n",
      "\t False positive: 0.00171765470498\n",
      "\t False negative: 0.0823811071246\n",
      "\t Correct pass: 0.881720572728\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(neighbors.KNeighborsClassifier(10, weights='distance'), trainingSize)\n",
    "printSummary('kNN, k=10, distance weighted', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN, k=100, distance weighted\n",
      "\t Correct flags: 0.0335639013973\n",
      "\t False positive: 0.00169112721919\n",
      "\t False negative: 0.0829978711693\n",
      "\t Correct pass: 0.881747100214\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(neighbors.KNeighborsClassifier(100, weights='distance'), trainingSize)\n",
    "printSummary('kNN, k=100, distance weighted', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN, k=1000, distance weighted\n",
      "\t Correct flags: 0.052351993209\n",
      "\t False positive: 0.0181050090525\n",
      "\t False negative: 0.0642097793576\n",
      "\t Correct pass: 0.865333218381\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(neighbors.KNeighborsClassifier(1000, weights='distance'), trainingSize)\n",
    "printSummary('kNN, k=1000, distance weighted', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance weighting improves performance, suggesting some clustering of flagged data in the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1: Nearest Centroid\n",
    "\n",
    "[Nearest centroid](http://scikit-learn.org/stable/modules/neighbors.html#nearest-centroid-classifier) is a subset of NN algorithms. Scikit-learn advertises it as a good baseline classifier, for its lack of parameterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Centroid\n",
      "\t Correct flags: 0.0600250684741\n",
      "\t False positive: 0.0624457015525\n",
      "\t False negative: 0.0565367040925\n",
      "\t Correct pass: 0.820992525881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(NearestCentroid(), trainingSize)\n",
    "printSummary('Nearest Centroid', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparable to kernel ridge and QDA, but with a slightly lower false positive rate. Shrunken threshold approaches do not yield substantially different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6: Decision Trees\n",
    "\n",
    "A major class of classifiers are [decision trees](http://scikit-learn.org/stable/modules/tree.html#classification), which we examine here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "\t Correct flags: 0.0545604064011\n",
      "\t False positive: 0.0183304926817\n",
      "\t False negative: 0.0620013661655\n",
      "\t Correct pass: 0.865107734752\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(tree.DecisionTreeClassifier(), trainingSize)\n",
    "printSummary('Decision Tree', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In line with some of the best classifiers examined so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.7: Naive Bayes Models\n",
    "\n",
    "Next we explore the collection of [naive Baysian models](http://scikit-learn.org/stable/modules/naive_bayes.html) provided by scikit-learn.\n",
    "\n",
    "### 1.7.1: Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with linear kernel\n",
      "\t Correct flags: 0.0339684455557\n",
      "\t False positive: 0.00353478748168\n",
      "\t False negative: 0.0825933270109\n",
      "\t Correct pass: 0.879903439952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(GaussianNB(), trainingSize)        \n",
    "printSummary('SVM with linear kernel', TT/(len(truth)-trainingSize), TF/(len(truth)-trainingSize), FT/(len(truth)-trainingSize), FF/(len(truth)-trainingSize))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2: Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with linear kernel\n",
      "\t Correct flags: 0.0327813405665\n",
      "\t False positive: 0.00144574797562\n",
      "\t False negative: 0.0837804320001\n",
      "\t Correct pass: 0.881992479458\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(MultinomialNB(), trainingSize)        \n",
    "printSummary('SVM with linear kernel', TT/(len(truth)-trainingSize), TF/(len(truth)-trainingSize), FT/(len(truth)-trainingSize), FF/(len(truth)-trainingSize))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.3: Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with linear kernel\n",
      "\t Correct flags: 0.0349234350441\n",
      "\t False positive: 0.00250684740727\n",
      "\t False negative: 0.0816383375225\n",
      "\t Correct pass: 0.880931380026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "TT, TF, FT, FF = runClassifier(BernoulliNB(), trainingSize)        \n",
    "printSummary('SVM with linear kernel', TT/(len(truth)-trainingSize), TF/(len(truth)-trainingSize), FT/(len(truth)-trainingSize), FF/(len(truth)-trainingSize))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Bayes models produce comparable results, all poorer than the SVM. The fundamental assumption of independent features made in these models is probably a poor one for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ensemble Methods\n",
    "\n",
    "In Part 1, we examined individual scikit-learn classifiers, and found that many of them provide similar performance, flagging about half of datasets that ought to be flagged. In this section, we explore ideas for combining the results of several of these classifiers into a final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Naive Polling\n",
    "\n",
    "First, we consider the simplest possible techniques: given decisions from our favorite classifiers, what is the majority opinion, and what is the performance of flagging a profile if any of the underlying classifiers have flagged it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Poll\n",
      "\t Correct flags: 0.060157705903\n",
      "\t False positive: 0.0626910807961\n",
      "\t False negative: 0.0564040666636\n",
      "\t Correct pass: 0.820747146637\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ed873d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.qda import QDA\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#load and arrange data\n",
    "truth, rawResults = reloadData()\n",
    "data = transpose(rawResults) #arrange data into rows by profile for consumption by scikit-learn\n",
    "shuffleLists(data, truth)    #randomize order of profiles\n",
    "    \n",
    "clf_SVM = svm.SVC(kernel='linear')\n",
    "clf_QDA = QDA()\n",
    "clf_KernelRidge = KernelRidge(kernel='linear')\n",
    "clf_SGD = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "clf_NearestCentroid = NearestCentroid()\n",
    "clf_DecisionTree = tree.DecisionTreeClassifier()\n",
    "\n",
    "clfs = [clf_SVM, clf_QDA, clf_KernelRidge, clf_SGD, clf_NearestCentroid, clf_DecisionTree]\n",
    "histEntries = []\n",
    "\n",
    "# train the classifiers\n",
    "for clf in clfs:\n",
    "    clf.fit(data[0:trainingSize], truth[0:trainingSize])\n",
    "    \n",
    "# poll classifiers and report\n",
    "TT = 0.\n",
    "TF = 0.\n",
    "FT = 0.\n",
    "FF = 0.\n",
    "\n",
    "for i in range(trainingSize, len(truth)):\n",
    "        flagsRaised = 0\n",
    "        for clf in clfs:\n",
    "            if clf.predict(data[i]):\n",
    "                flagsRaised += 1\n",
    "                \n",
    "        histEntries.append(flagsRaised)\n",
    "    \n",
    "        assessment = flagsRaised >= len(clfs)/2\n",
    "            \n",
    "        if assessment and truth[i]:\n",
    "            TT += 1\n",
    "        elif assessment and not truth[i]:\n",
    "            TF += 1\n",
    "        elif not assessment and truth[i]:\n",
    "            FT += 1\n",
    "        elif not assessment and not truth[i]:\n",
    "            FF += 1 \n",
    "    \n",
    "printSummary('Majority Poll', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))\n",
    "\n",
    "plt.hist(histEntries, bins=[0,1,2,3,4,5,6,7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the number of times n classifiers flagged a given dataset. Another simple approach is to flag a dataset if *any* of the classifiers flag it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any Flag\n",
      "\t Correct flags: 0.0637919714564\n",
      "\t False positive: 0.0845563609595\n",
      "\t False negative: 0.0527698011102\n",
      "\t Correct pass: 0.798881866474\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.qda import QDA\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn import tree\n",
    "\n",
    "#load and arrange data\n",
    "truth, rawResults = reloadData()\n",
    "data = transpose(rawResults) #arrange data into rows by profile for consumption by scikit-learn\n",
    "shuffleLists(data, truth)    #randomize order of profiles\n",
    "    \n",
    "clf_SVM = svm.SVC(kernel='linear')\n",
    "clf_QDA = QDA()\n",
    "clf_KernelRidge = KernelRidge(kernel='linear')\n",
    "clf_SGD = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "clf_NearestCentroid = NearestCentroid()\n",
    "clf_DecisionTree = tree.DecisionTreeClassifier()\n",
    "\n",
    "clfs = [clf_SVM, clf_QDA, clf_KernelRidge, clf_SGD, clf_NearestCentroid, clf_DecisionTree]\n",
    "\n",
    "# train the classifiers\n",
    "for clf in clfs:\n",
    "    clf.fit(data[0:trainingSize], truth[0:trainingSize])\n",
    "    \n",
    "# poll classifiers and report\n",
    "TT = 0.\n",
    "TF = 0.\n",
    "FT = 0.\n",
    "FF = 0.\n",
    "\n",
    "for i in range(trainingSize, len(truth)):\n",
    "        assessment = False\n",
    "        for clf in clfs:\n",
    "            assessment = assessment or clf.predict(data[i])\n",
    "            \n",
    "        if assessment and truth[i]:\n",
    "            TT += 1\n",
    "        elif assessment and not truth[i]:\n",
    "            TF += 1\n",
    "        elif not assessment and truth[i]:\n",
    "            FT += 1\n",
    "        elif not assessment and not truth[i]:\n",
    "            FF += 1 \n",
    "    \n",
    "printSummary('Any Flag', TT/(datasetSize-trainingSize), TF/(datasetSize-trainingSize), FT/(datasetSize-trainingSize), FF/(datasetSize-trainingSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, flagging any profile that is flagged by an individual classifier performs better than the majority poll, but this performance is no better than Kernel Ridge or Quadratic Discriminant Analysis alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
